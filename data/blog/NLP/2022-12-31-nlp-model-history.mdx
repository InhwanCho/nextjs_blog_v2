---
title:  'Seq to Seq & Attention & Transformer'


tags:
  - nlp
  - seq2seq
  - attention
  - transformer

date: 2022-12-31
lastmod: 2023-01-05
 
---


## 기계 번역 모델의 발전

- 참고 : [위키 독스](https://wikidocs.net/31379)
- 참고 : [Jay Almmar 자료](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

![trans](https://user-images.githubusercontent.com/111936229/210937926-adb9b2dc-c4a6-4dcc-b354-9639f51489df.png)

- GPT : `Transformer`의 `디코더 아키텍쳐`를 활용
- BERT : `Transformer`의 `인코더 아키텍쳐`를 활용

## Seq2Seq2(시퀀스 투 시퀀스)

<br/>
<br/>
- `[인코더 - 컨텍스트백터 - 디코더]`로 이루어져 있고
- seq2seq는 인코더와 디코더 아키텍처의 내부는 사실 `두 개의 RNN 구조`로 이루어진 모델
- 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)는 챗봇과 기계 번역에서 많이 사용됩니다.
- 대략적인 구조는 `입력 시퀀스`(질문)와 `출력 시퀀스`(대답)으로 이루어져 있습니다.


  1) 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해 하나의 벡터로 만듭니다. 
  이를 컨텍스트 벡터(context vector)라고 부릅니다.(이는 고정된 크기라서 한계점이 존재)
   
  2) `입력 문장의 정보`가 하나의 `컨텍스트 벡터`로 모두 압축되면 벡터를 `디코더`로 전송합니다. 

  3) 디코더는 번역된 단어를 순차적으로 출력합니다.

<br/>

- 아래 동영상을 보시면 쉽게 이해될 것입니다(10초)


<video controls width="800">
    <source src="/assets/videos/seq2seq.mp4" type="video/mp4"/>
</video>


   

## seq2seq의 한계점

- 항상 고정된 크기의 벡터에(컨텍스트 벡터) 모든 정보를 저장하기 때문에 bottle neck현상이 발생하여 정보 손실이 발생한다.
- 입력의 길이가 길어지면 기울기 소실 문제가 발생한다.
<br/>(context vector를 기준으로 Encoder, Decoder가 완전히 분리되어 있으므로 입출력의 연관 관계가 너무 떨어져 있어서 역전파 시 기울기 소실 발생)

## 어텐션 모델(Attention)

- seq2seq의 문제점을 해결하기 위한 모델이 Attention 모델이다.
- 어텐션은 중요한 부분에 더 `attention(집중)`하자는 요지로 만들어진 모델이며, 간단히 말해서 인코딩의 모든 은닉층의 정보를 디코더로 전달하는 것이다.<br/>(Encoder의 hidden state를 Decoder에서도 사용하는 방법)
- 단어에 가중치를 주는 레이어가 추가됨(seq2seq에 어텐션 레이어가 추가됨)

- 아래의 동영상(10초)은 seq2seq에 attention을 추가한 동영상입니다.
  
<video controls width="800">
    <source src="/assets/videos/attention.mp4" type="video/mp4"/>
</video>



## 트랜스포머(transformer)

- 앞의 어텐션을 RNN의 보정 용도가 아닌 어텐션만으로 인코더와 디코더를 만든 모델이 트렌스포머!  
(이 모델이 등장하고 자연어 분야의 생태계가 변했습니다)
- `인코더와 디코더라는 단위가 여러 개로 구성되는 구조`입니다. 트랜스포머를 제안한 논문에서는 인코더와 디코더의 개수를 각각 6개 사용하였습니다.
- 디코더는 seq2seq 구조처럼 시작 토큰 \<sos\>부터 종료 토큰 \<eos\>까지 연산을 진행합니다. 이는 RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음을 보여줍니다.
- 하지만 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있는데 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는데, 이를 `포지셔널 인코딩(positional encoding)`이라고 합니다.

- 인코더는 총 레이어 개수 만큼의 층 연산을 순차적으로 한 후에 마지막 층의 인코더의 출력을 디코더에게 전달합니다. 
  
- 인코더 연산이 끝났으면 디코더 연산이 시작되어 디코더 또한 만큼의 연산을 하는데, 이때마다 인코더가 보낸 출력을 각 디코더 층 연산에 사용합니다.

- 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됩니다. 

- 트랜스포머의 디코더에서는 현재 시점의 예측에서 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크(look-ahead mask)를 도입했습니다.

