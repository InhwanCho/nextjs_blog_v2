---
title: BERT(Bidirectional Encoder Representations Transformer)


tags:
- paper
- NLP

date: 2023-08-15
lastmod: 2023-08-15
 
---



## BERT 개요

- GPT : `Transformer`의 `디코더 아키텍쳐`를 활용
- BERT : `Transformer`의 `인코더 아키텍쳐`를 활용

BERT(Bidirectional Encoder Representations Transformer)는 딥러닝 언어 모델로, 트랜스포머의 인코더 부분을 기반으로 합니다.
이 모델은 양방향으로 문맥을 이해하며, 텍스트의 의미를 더 정확하게 파악하는 데 도움을 줍니다.
기본적으로 BERT는 Fine-tuning을 위해 만들어진 `Pre-trained model`입니다. (==fine-tuning approach)
주로 `Fine-tuning을 통해 특정 작업에 적용`되며, 자연어 처리 분야에서 다양한 언어 관련 작업에서 높은 성능을 보여줍니다.
BERT의 등장은 언어 모델의 발전을 대표하는 중요한 마일스톤으로 평가되고 있습니다.

<img width="650" alt="BERT 구조" src="https://github.com/InhwanCho/InhwanCho.github.io/assets/111936229/dab3b2c3-b0c1-4cbe-85a7-7044f07c67cb"/>

위의 그림은 BERT논문에서 가져온 그림으로 BERT를 잘 설명해주고 있습니다.

---

Pre-training이 어떻게 되는지 자세하게 살펴보겠습니다.

<img width="250" alt="Transformer의 Encoder block" src="https://github.com/InhwanCho/InhwanCho.github.io/assets/111936229/4b4b90e5-9d7d-4e84-b6b4-5a6eee54b117"/>

BERT의 model architecture를 살펴보면 위의 그림의 Transformer의 encoder block을 사용합니다.

---

다음으로 BERT의 input값이 어떻게 들어가는지 살펴보겠습니다.

<img width="500" alt="BERT의 input" src="https://github.com/InhwanCho/InhwanCho.github.io/assets/111936229/1016d7cd-c23e-44c8-9535-baa333564c78"/>

논문의 그림입니다. Input 값은 3가지 embeddings의 합입니다.
`Token Embeddings`, `Segment Embeddings`,`Postion Embeddings` 입니다.

먼저 `Token Embeddings`은 특징은 `WordPiece`를 30,000개의 vocab을 사용한 embedding입니다.

    WordPiece는 문장을 공백 문자(space)단위로 나누는게 아닌 
    특징 단어(ex playing -> [play, ##ing])로 나누어 보다 의미있는 학습을 도와줍니다.

`Segment Embeddings`은 문장 A와 문장 B를 구분해서 embedding하는 것입니다.

마지막 `Postion Embeddings`은 transformer에 사용된 것과 동일하며 단어들의 포지션을 알려주는 embedding입니다.

이 3가지 임베딩의 합이 input으로 사용됩니다.

---

BERT-base를 기준으로 layer의 수는 12개, hidden size는 768, Attention heads는 12개, parameters는 110M개 입니다.

<img width="500" alt="BERT pre-training" src="https://github.com/InhwanCho/InhwanCho.github.io/assets/111936229/2db35731-4cf2-47f9-93f8-a5a1294ab012"/>

Input data는 `Unlabeled Sentence A and B Pair`가 사용되었습니다.
다시 말해, 라벨이 없는 일반적인 문장 데이터가 활용되었습니다. 이는 두 개의 문장이 쌍으로 구성될 수 있으며(예: QnA 문장), 또는 하나의 단일 문장으로 구성될 수도 있습니다.

여기서 sentence는 Masked sentence A, B로 들어오는데, A와 B는 스페셜 토큰 중 1개인 \[SEP]를 통해 문장이 구분 됩니다. 또한 맨 앞에 보이는 \[CLS]토큰을 통해 문장의 시작을 알 수 있습니다.

게다가 Masked라는 단어에서 유추할 수 있겠지만, 일정 비율만큼 Masking을 하여 사용한다는 의미입니다.

그리고 final hidden vector의 맨 첫번째 토큰(그림에서는 맨 위의 대문자 C) 또한 \[CLS] 토큰이며 이는 이를 활용하여 `감성 분류`인지 `다음 문장 예측`인지 등을 확인하는 토큰입니다.

이 input 문장들을 Task 2가지를 사용합니다. task 1 :Masked Language Model(MLM)과 task 2 :Next Sentence Prediction(NSP)을 사용하여 Pre-training을 합니다.

`MLM`이란, input tokens의 일정 비율(15%)을 마스킹하고 마스킹 된 토큰을 예측하는 것이고,
`NSP`는 A, B 문장 사이의 관계가 B가 A의 다음(next) 문장인지 아닌지 학습합니다.

MLM을 자세히 살펴보겠습니다.

<img width="650" alt="MLM" src="https://github.com/InhwanCho/InhwanCho.github.io/assets/111936229/85ab03dd-dd82-43e7-bcce-d8ac92e48688"/>

위의 그림은 MLM을 설명하는 그림입니다.

input은 [$W_1$, $W_2$, $W_3$, $W_4$, $W_5$]인데 일정 %(15%)만큼을 \[MASK] 토큰으로 바꿔줍니다. 그림에서는 $W_4$가 마스킹되어 학습이 진행됩니다.

즉, 여기서는 output값인 $O_4$를 FC layer+GELU+Norm을 통해 나온 $W^\prime_4$가 $W_4$가 되도록 학습한다는 의미입니다.

다만, 논문에서는 이러한 방식을 통해 pre-trained model은 얻을 수 있지만, fine-tuning 사이의 mismatch가 발생하고 이를 해결 할 필요가 있다고 합니다.

이를 해결하기위해 위에서 언급한 %(15%)의 \[MASK] token에서 추가적인 처리를 더 해줘야 합니다.

    15%의 마스킹된 것의 

    `80%`는 token을 `[MASK] token`으로 변경.
 
    `10%`는 token을 `random word`로 변경.

    나머지 `10%`는 token을 `원래 단어 그대로` 놔둡니다.

위의 방식을 통해 BERT의 `pre-trained model`을 얻을 수 있습니다.

---

마지막으로 BERT의 `Fine-tuning`에 대해 알아보겠습니다.

<img width="700" alt="BERT fine-tuning" src="https://github.com/InhwanCho/InhwanCho.github.io/assets/111936229/f935c7be-16c0-403a-954a-765852bb19a2"/>

BERT의 파인튜닝에는 크게 4가지가 있습니다.

(a)는 문장 A, B가 들어왔을때 단순히 특정한 Class에 속하는지 아닌지는 이 구조(범주의 예측)

(b)는 문장 1개가 들어왔을때 특정한 Class에 속하는지 아닌지는 이 구조(범주의 예측)

(c)는 문장 Question, Answer를 맞추는 구조입니다.(answer 문장의 생성)

(d)는 문장 1개가 들어왔을때 객체명 인식, 형태소 분석 등을 하는 구조입니다.

### 참고 자료

[Transformer 논문](https://arxiv.org/abs/1706.03762 "Transformer 논문")  
[BERT paper](https://arxiv.org/abs/1810.04805?source=post_page "bert paper")  
[고려대 유튜브](https://www.youtube.com/watch?v=o_Wl29aW5XM&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm&index=19 "고려대 유튜브")  
[Jay alammar 블로그](https://jalammar.github.io/illustrated-gpt2/ "Jay alammar blog")
