---
title:  '스케일링(Scaling)'

tags:
  - sklearn
  - preprocessing
  - scaling


date: 2022-11-21
lastmod: 2022-11-21
 
---

## 차원축소 (비지도 학습) 중 스케일링을 하는 이유

### 차원 축소를 하는 이유

- 고차원 데이터(columns)의 의미적 특성 -> 최대한 유지 -> 저차원 축소 표현  
shape이 M x N 행렬에서(데이터프레임) PCA를 이용하여 N차원 -> (N보다 작은)차원
- 차원의 저주를 피하기 위해 : outfitting을 피하기 위해
- 변수간 척도가 다름 -> 비교가 불가능, 모수의 왜곡 -> 변수의 표준화 작업이 필요(표준 정규분포로 표준화 등)
- 데이터 스케일링으로는 StandardScale, MinmaxScale, RobustScale, log1p 등이 있다.

### 스케일링 예시

1.표준화(StandardScaling)

```python
# 표준화 방법은 아래와 같습니다.
import numpy as np
import scipy.stats as ss
from sklearn.preprocessing import StandardScaler
data = np.random.randint(30, size=(6,5))
# (data - mean)/std 아래 3가지 방법 모두 동일한 결과값을 나타냄
(data-np.mean(data, axis=0))/np.std(data,axis=0)
== ss.zscore(data) ==StandardScaler().fit_transform(data) 
# 결과값
array([[-1.2265705 ,  0.67625223,  0.90236202,  1.28902742, -0.9166985 ],
       [-1.2265705 , -0.13525045, -1.44108561, -0.60505369, -0.9166985 ],
       [-0.19967427, -1.4877549 , -1.36027707,  1.28902742, -0.64168895],
       [ 1.51181945,  1.21725401,  0.82155348, -1.39425415,  1.74172714],
       [ 0.48492322,  0.81150267,  0.49831932,  0.02630668, -0.1833397 ],
       [ 0.65607259, -1.08200356,  0.57912786, -0.60505369,  0.9166985 ]]) 
```

2.RobustScaler

- 표준화(이상치가 적다는 가정에서 수행),  but 만약 이상치가 많다면?
- 이상치를 제거후에 표준화
- 평균 대신 덜 민감한 중위수, 표준편차 대신 IQR
(data - mean)/std => (data - median)/IQR == RobustScaler

```python
# (data - median)/IQR == RobustScaler 이상치를 제거하고 mean대신 median값을 빼줌
# 그래프 그려보면 표준화보다 이쁘게 나옴
from sklearn.preprocessing import RobustScaler
RobustScaler().fit_transform(data) 
# 결과값
array([[-1.2265705 ,  0.67625223,  0.90236202,  1.28902742, -0.9166985 ],
       [-1.2265705 , -0.13525045, -1.44108561, -0.60505369, -0.9166985 ],
       [-0.19967427, -1.4877549 , -1.36027707,  1.28902742, -0.64168895],
       [ 1.51181945,  1.21725401,  0.82155348, -1.39425415,  1.74172714],
       [ 0.48492322,  0.81150267,  0.49831932,  0.02630668, -0.1833397 ],
       [ 0.65607259, -1.08200356,  0.57912786, -0.60505369,  0.9166985 ]])
```

3.MinMaxScaler

```python
from sklearn.preprocessing import MinMaxScaler, minmax_scale
MinMaxScaler().fit_transform(data)
minmax_scale(data, axis=0) #위와 동일
array([[0.        , 0.8       , 1.        , 1.        , 0.        ],
       [0.        , 0.5       , 0.        , 0.29411765, 0.        ],
       [0.375     , 0.        , 0.03448276, 1.        , 0.10344828],
       [1.        , 1.        , 0.96551724, 0.        , 1.        ],
       [0.625     , 0.85      , 0.82758621, 0.52941176, 0.27586207],
       [0.6875    , 0.15      , 0.86206897, 0.29411765, 0.68965517]])
```

4.log1p

- 그냥 log가 아닌 1p를 사용하는 이유는 0에 가까운 아주 작은 양수의 경우 (ex. 0.00001) 음의 무한대에 가까워지게 된다. 즉 -inf가 나오게되기에 1을 더한 log1p를 사용한다

```python
import numpy as np
df = np.log1p(df)
np.log1p(data) 
```
